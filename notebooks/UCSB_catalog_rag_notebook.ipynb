{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCSB Course Catalog RAG System\n",
        "## RapidFire AI Winter Competition - RAG Track\n",
        "\n",
        "**Dataset:** UCSB 2009-2010 Course Catalog\n",
        "**Research Question:** Which chunking and reranking strategies optimize retrieval for course catalog queries?\n",
        "**Configurations Tested:** 4 (chunk_size: [128, 256] √ó reranking top_n: [2, 5])\n",
        "\n",
        "## Experiment Hypothesis\n",
        "\n",
        "Larger chunks (256 tokens) will better preserve course description context compared to smaller chunks (128 tokens), leading to improved Precision and NDCG@5 scores.\n",
        "Keeping more reranked documents (top_n=5) will improve Recall at the cost of some Precision."
      ],
      "metadata": {
        "id": "-0qAgvKsZBKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import rapidfireai\n",
        "    print(\"‚úÖ rapidfireai already installed\")\n",
        "except ImportError:\n",
        "    %pip install rapidfireai\n",
        "    !rapidfireai init --evals"
      ],
      "metadata": {
        "id": "2uUDsBt62kGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HgtA03Wg2sWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
        "import re, json\n",
        "from typing import List as listtype, Dict, Any"
      ],
      "metadata": {
        "id": "ynr6LCq2ZOyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Dataset Preparation\n",
        "\n",
        "The UCSB 2009-2010 catalog was processed into three files:\n",
        "- **corpus.jsonl**: 1,304 document chunks from the 490-page PDF\n",
        "- **queries.jsonl**: 30 typical student questions about courses and requirements\n",
        "- **qrels.tsv**: 150 relevance judgments (5 per query) generated via embedding similarity\n",
        "\n",
        "For dataset preparation code, see the separate data preparation scripts."
      ],
      "metadata": {
        "id": "hIYnHTBVZSk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_dir = Path(\"/content/tutorial_notebooks/rag-contexteng/datasets\")\n",
        "\n",
        "# Load UCSB catalog queries\n",
        "ucsb_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=str(dataset_dir / \"ucsb_catalog\" / \"queries.jsonl\"),\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Load relevance labels\n",
        "qrels = pd.read_csv(str(dataset_dir / \"ucsb_catalog\" / \"qrels.tsv\"), sep=\"\\t\")\n",
        "\n",
        "# Sample 50% of queries for this experiment\n",
        "sample_fraction = 0.5\n",
        "rseed = 1\n",
        "random.seed(rseed)\n",
        "\n",
        "sample_size = int(len(ucsb_dataset) * sample_fraction)\n",
        "ucsb_dataset = ucsb_dataset.shuffle(seed=rseed).select(range(sample_size))\n",
        "\n",
        "query_ids = set([int(qid) for qid in ucsb_dataset[\"query_id\"]])\n",
        "qrels = qrels[qrels[\"query_id\"].isin(query_ids)]\n",
        "\n",
        "print(f\"Using {len(ucsb_dataset)} queries ({sample_fraction*100}% of dataset)\")\n",
        "print(f\"Filtered qrels to {len(qrels)} relevance judgments\")"
      ],
      "metadata": {
        "id": "0_wVyfJzZWuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß RAG Pipeline Configuration\n",
        "\n",
        "Testing 4 configurations:\n",
        "\n",
        "| Config | Chunk Size | Rerank Top-N | Hypothesis |\n",
        "|--------|-----------|--------------|------------|\n",
        "| 1 | 256 | 2 | High precision, focused context |\n",
        "| 2 | 256 | 5 | Balanced precision & recall |\n",
        "| 3 | 128 | 2 | Granular chunks, focused results |\n",
        "| 4 | 128 | 5 | Granular chunks, broad coverage |\n",
        "\n",
        "**Fixed Parameters:**\n",
        "- Embedding: sentence-transformers/all-MiniLM-L6-v2\n",
        "- Initial retrieval: k=8 (similarity search)\n",
        "- Reranker: cross-encoder/ms-marco-MiniLM-L6-v2\n",
        "- Generator: Qwen/Qwen2.5-0.5B-Instruct"
      ],
      "metadata": {
        "id": "o2ycIjnoZc-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "rag_gpu = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        path=str(dataset_dir / \"ucsb_catalog\"),\n",
        "        glob=\"corpus.jsonl\",\n",
        "        loader_cls=JSONLoader,\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"content_key\": \"text\",\n",
        "            \"metadata_func\": lambda record, metadata: {\n",
        "                \"corpus_id\": int(record.get(\"_id\"))\n",
        "            },\n",
        "            \"json_lines\": True,\n",
        "            \"text_content\": False,\n",
        "        },\n",
        "        sample_seed=42,\n",
        "    ),\n",
        "    # EXPERIMENT VARIABLE 1: Chunk size\n",
        "    text_splitter=List([\n",
        "        RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            encoding_name=\"gpt2\", chunk_size=256, chunk_overlap=32\n",
        "        ),\n",
        "        RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            encoding_name=\"gpt2\", chunk_size=128, chunk_overlap=32\n",
        "        ),\n",
        "    ]),\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "    },\n",
        "    vector_store=None,\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 8},\n",
        "    # EXPERIMENT VARIABLE 2: Reranking strategy\n",
        "    reranker_cls=CrossEncoderReranker,\n",
        "    reranker_kwargs={\n",
        "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cpu\"},\n",
        "        \"top_n\": List([2, 5]),\n",
        "    },\n",
        "    enable_gpu_search=True,\n",
        ")"
      ],
      "metadata": {
        "id": "BnsftBnHZgXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_preprocess_fn(\n",
        "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
        ") -> Dict[str, listtype]:\n",
        "    \"\"\"Retrieve context and format prompts for LLM\"\"\"\n",
        "\n",
        "    INSTRUCTIONS = \"\"\"You are a helpful academic advisor for UCSB students.\n",
        "Use the provided course catalog information to answer questions about courses,\n",
        "requirements, policies, and academic programs. Be specific and cite relevant\n",
        "catalog sections when possible.\"\"\"\n",
        "\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "    retrieved_documents = [\n",
        "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
        "    ]\n",
        "\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
        "\n",
        "    return {\n",
        "        \"prompts\": [\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"\"\"Here is relevant information from the UCSB course catalog:\n",
        "\n",
        "{context}\n",
        "\n",
        "Based on this catalog information, please answer the following question:\n",
        "{question}\"\"\",\n",
        "                },\n",
        "            ]\n",
        "            for question, context in zip(batch[\"query\"], serialized_context)\n",
        "        ],\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "        **batch,\n",
        "    }"
      ],
      "metadata": {
        "id": "D-B9XnmLZkDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
        "    \"\"\"Attach ground truth documents for evaluation\"\"\"\n",
        "    batch[\"ground_truth_documents\"] = [\n",
        "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
        "        for query_id in batch[\"query_id\"]\n",
        "    ]\n",
        "    return batch"
      ],
      "metadata": {
        "id": "eAHs8hUSZn26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
        "    \"\"\"Compute NDCG@k metric\"\"\"\n",
        "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
        "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
        "\n",
        "    ideal_length = min(k, len(expected_docs))\n",
        "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
        "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
        "    \"\"\"Compute Reciprocal Rank for a single query\"\"\"\n",
        "    rr = 0\n",
        "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
        "        if retrieved_doc in expected_docs:\n",
        "            rr = 1 / (i + 1)\n",
        "            break\n",
        "    return rr\n",
        "\n",
        "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Compute retrieval metrics per batch\"\"\"\n",
        "    precisions, recalls, f1_scores, ndcgs, rrs = [], [], [], [], []\n",
        "    total_queries = len(batch[\"query\"])\n",
        "\n",
        "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
        "        expected_set = set(gt)\n",
        "        retrieved_set = set(pred)\n",
        "\n",
        "        true_positives = len(expected_set.intersection(retrieved_set))\n",
        "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
        "        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
        "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
        "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
        "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
        "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
        "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
        "    }\n",
        "\n",
        "def sample_accumulate_metrics_fn(\n",
        "    aggregated_metrics: Dict[str, listtype],\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Accumulate metrics across all batches\"\"\"\n",
        "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
        "    total_queries = sum(num_queries_per_batch)\n",
        "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        **{\n",
        "            metric: {\n",
        "                \"value\": sum(\n",
        "                    m[\"value\"] * queries\n",
        "                    for m, queries in zip(\n",
        "                        aggregated_metrics[metric], num_queries_per_batch\n",
        "                    )\n",
        "                ) / total_queries,\n",
        "                \"is_algebraic\": True,\n",
        "                \"value_range\": (0, 1),\n",
        "            }\n",
        "            for metric in algebraic_metrics\n",
        "        },\n",
        "    }"
      ],
      "metadata": {
        "id": "JiXkKEJPZtzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_config1 = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.25,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "        \"distributed_executor_backend\": \"mp\",\n",
        "        \"enable_chunked_prefill\": False,\n",
        "        \"enable_prefix_caching\": False,\n",
        "        \"max_model_len\": 3000,\n",
        "        \"disable_log_stats\": True,\n",
        "        \"enforce_eager\": True,\n",
        "        \"disable_custom_all_reduce\": True,\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.8,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 128,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        "    prompt_manager=None,\n",
        ")\n",
        "\n",
        "batch_size = 3\n",
        "\n",
        "config_set = {\n",
        "    \"vllm_config\": vllm_config1,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"preprocess_fn\": sample_preprocess_fn,\n",
        "    \"postprocess_fn\": sample_postprocess_fn,\n",
        "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
        "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
        "    \"online_strategy_kwargs\": {\n",
        "        \"strategy_name\": \"normal\",\n",
        "        \"confidence_level\": 0.95,\n",
        "        \"use_fpc\": True,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "Cik3Ofo9Zxpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_group = RFGridSearch(config_set)"
      ],
      "metadata": {
        "id": "EIcdrs_VZz1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Running Experiments\n",
        "\n",
        "This cell executes all 4 configurations in parallel using RapidFire AI's experiment orchestration system.\n",
        "\n",
        "**Expected Runtime:** ~20-30 minutes on Colab T4 GPU"
      ],
      "metadata": {
        "id": "P8u33YMdZ1GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = Experiment(experiment_name=\"ucsb-catalog-rag-exp1\", mode=\"evals\")"
      ],
      "metadata": {
        "id": "DFj8chW-Z5PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(8855)"
      ],
      "metadata": {
        "id": "hBljaqSWZ7LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = experiment.run_evals(\n",
        "    config_group=config_group,\n",
        "    dataset=ucsb_dataset,\n",
        "    num_actors=1,\n",
        "    num_shards=4,\n",
        "    seed=42,\n",
        ")"
      ],
      "metadata": {
        "id": "ea_BQMFxZ85W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Results Analysis\n",
        "\n",
        "The table below shows the final metrics for all 4 configurations.\n",
        "\n",
        "**Key Metrics:**\n",
        "- **Precision**: Of retrieved documents, what % were relevant?\n",
        "- **Recall**: Of all relevant documents, what % were retrieved?\n",
        "- **F1 Score**: Harmonic mean of Precision and Recall\n",
        "- **NDCG@5**: How well were relevant documents ranked? (higher = better ranking)\n",
        "- **MRR**: How quickly was the first relevant document found?"
      ],
      "metadata": {
        "id": "4qStjuGuZ-SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame([\n",
        "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v\n",
        "     for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
        "    for run_id, (_, metrics_dict) in results.items()\n",
        "])\n",
        "\n",
        "# Display key columns only for clarity\n",
        "display_cols = ['run_id', 'chunk_size', 'top_n', 'Precision', 'Recall', 'F1 Score', 'NDCG@5', 'MRR']\n",
        "results_df[display_cols]"
      ],
      "metadata": {
        "id": "yCOkNkZNaA0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Key Findings\n",
        "\n",
        "### Best Configuration: [Fill this in based on your results]\n",
        "\n",
        "**Observations:**\n",
        "- Chunk size impact: [Compare 256 vs 128]\n",
        "- Reranking strategy impact: [Compare top_n=2 vs top_n=5]\n",
        "- Trade-offs observed: [Precision vs Recall, etc.]\n",
        "\n",
        "### RapidFire AI Value\n",
        "\n",
        "RapidFire AI enabled:\n",
        "1. **Parallel execution**: All 4 configs ran simultaneously, saving ~XX minutes\n",
        "2. **Live metrics**: Real-time confidence intervals showed convergence\n",
        "3. **Interactive control**: Could stop/clone configs dynamically\n",
        "4. **Easy reproducibility**: Complete config tracking in logs"
      ],
      "metadata": {
        "id": "9dTosiBYaEnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.end()\n",
        "print(\"Experiment completed successfully!\")"
      ],
      "metadata": {
        "id": "kDbHkaZPaJNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Google Drive for easy access\n",
        "results_df.to_csv('/content/drive/MyDrive/ucsb_catalog_results.csv', index=False)\n",
        "print(\"‚úÖ Results saved to Google Drive\")"
      ],
      "metadata": {
        "id": "zdZ-qDwOaMS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}